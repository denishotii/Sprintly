# ===========================================
# Seed Agent Configuration
# ===========================================
# Copy this file to .env and fill in your values

# ===========================================
# Required: LLM API Keys (direct OpenAI and/or Anthropic — no OpenRouter)
# ===========================================
# At least one provider key is required. Set PRIMARY_PROVIDER to choose which to use by default.

# OpenAI API key — https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Anthropic API key — https://console.anthropic.com/
ANTHROPIC_API_KEY=

# Which provider to use: "openai" or "anthropic" (default: anthropic)
PRIMARY_PROVIDER=anthropic

# Model per provider (optional; defaults shown)
OPENAI_MODEL=gpt-4o
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# Pipeline per-step models (planner → builder → verifier). These are the only source of truth; no hardcoded defaults in code.
# If unset, the primary provider's model (OPENAI_MODEL or ANTHROPIC_MODEL) is used for all steps.
# Recommended: Opus for planning, Sonnet for high-complexity builds, Haiku for fast low/medium builds.
# claude-* = Anthropic, gpt-* = OpenAI.
PLANNER_MODEL=claude-opus-4-6
BUILDER_MODEL=claude-sonnet-4-6
# Fast model for low/medium complexity (Haiku is ~4x faster than Sonnet; used automatically by the pipeline)
BUILDER_FAST_MODEL=claude-haiku-4-5-20251001
VERIFIER_MODEL=claude-sonnet-4-6
TEXT_RESPONSE_MODEL=gpt-4o

# Maximum tokens to generate per response
MAX_TOKENS=4096

# Builder token caps per complexity tier:
#   low    → simple landing pages, static sites   (fast; ~8-20s builder time)
#   medium → interactive sites, small apps        (balanced; ~20-40s builder time)
#   high   → data apps, complex UIs               (quality; uses BUILDER_MODEL)
BUILDER_LOW_MAX_TOKENS=8000
BUILDER_MEDIUM_MAX_TOKENS=14000
BUILDER_MAX_TOKENS=20000

# Temperature for response generation (0.0 - 2.0)
TEMPERATURE=0.7

# ===========================================
# Required: Solana Wallet Address
# ===========================================
# Your Solana wallet address for receiving payments
SOLANA_WALLET_ADDRESS=YourSolanaWalletAddressHere

# ===========================================
# Optional: Seedstr API Key (auto-generated on register)
# ===========================================
# This will be populated automatically when you run: npm run register
SEEDSTR_API_KEY=

# ===========================================
# Optional: Agent Behavior
# ===========================================
# Minimum job budget to accept (in USD)
MIN_BUDGET=0.50

# Maximum number of concurrent jobs to process
MAX_CONCURRENT_JOBS=3

# Polling interval for new jobs (in seconds)
POLL_INTERVAL=30

# ===========================================
# Optional: Tool Configuration
# ===========================================
# Enable/disable specific tools (true/false)
TOOL_WEB_SEARCH_ENABLED=true
TOOL_CALCULATOR_ENABLED=true
TOOL_CODE_INTERPRETER_ENABLED=true

# Tavily API Key for web search (optional, uses fallback if not set)
# Get your key at: https://tavily.com
TAVILY_API_KEY=

# ===========================================
# Optional: Seedstr Platform
# ===========================================
# Seedstr API base URL (change for development)
SEEDSTR_API_URL=https://www.seedstr.io/api/v2

# ===========================================
# Optional: WebSocket (Real-time job notifications)
# ===========================================
# Enable WebSocket for instant job notifications (default: true)
# When enabled, the agent receives jobs immediately instead of waiting for the next poll
USE_WEBSOCKET=true

# Pusher credentials (required for WebSocket)
# Get these from your Seedstr dashboard or ask an admin
PUSHER_KEY=
PUSHER_CLUSTER=us2

# ===========================================
# Optional: Logging
# ===========================================
# Log level: debug, info, warn, error
LOG_LEVEL=info

# Enable verbose logging for debugging
DEBUG=false

# ===========================================
# Optional: LLM Retry Settings
# ===========================================
# These settings control retry behavior for recoverable LLM errors
# (e.g., malformed JSON tool arguments, truncated responses)

# Maximum number of retry attempts for recoverable errors
LLM_RETRY_MAX_ATTEMPTS=3

# Base delay between retries in milliseconds (uses exponential backoff)
LLM_RETRY_BASE_DELAY_MS=1000

# Maximum delay between retries in milliseconds
LLM_RETRY_MAX_DELAY_MS=10000

# Fall back to text-only response (no tools) if all retries fail
LLM_RETRY_FALLBACK_NO_TOOLS=true
